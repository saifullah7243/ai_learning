import re
import sys
import os
import asyncio
from typing import List, Dict, Any

from openai import AsyncOpenAI
from dotenv import load_dotenv
from rich.console import Console
from rich.markdown import Markdown
import json
from jsonschema import validate, ValidationError


# Rich console setup
console = Console()

# Load environment variables from .env
load_dotenv()

# Ensure project root (ai_agent_book/) is in sys.path
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(root_path)

# Import config loader from utils
from utils.config_loader import load_config
config = load_config()
openai_key = config.openai_key

# Setup OpenAI async client
client = AsyncOpenAI(api_key=openai_key)

## --- Basic output validation
def is_json_and_conforms_to_schema(output_text:str,schema:Dict)->bool:
    try:
        output_json = json.loads(output_text)
        validate(instance=output_json, schema=schema)
        return True
    except json.JSONDecodeError:
        print("Output Validation Faild: Not Valid Json")
        return False
    except ValidationError as e:
        print(f"Output Validation Failed: Does not confirm to schema. Error:{e.message}")
        return False

def contain_blacklisted_keywords(output_text:str, blacklist:List[str])->bool:
    for keyword in blacklist:
        if keyword.lower() in output_text.lower():
            print(f"Output validation failed: Contain blacklist keyword:{keyword}.")
            return True
        return False
    
## Example usage
PRODUCT_SCHEMA = { 
    "type": "object", 
    "properties": { 
        "product_name": {"type": "string"}, 
        "price": {"type": "number"}, 
        "in_stock": {"type": "boolean"} 
    }, 
    "required": ["product_name", "price", "in_stock"] 
} 

# Blacklist of words
CONTENT_BLACKLIST = ["password", "credit card", "social security number", "secret"] 

async def generate_and_validate_product_info(product_query:str):
    system_prompt = f""" 
        You are an assistant that provides product information in JSON format. 
        The JSON object must conform to this schema: 
        {json.dumps(PRODUCT_SCHEMA)} 
        Only provide information about the requested product. Do not include any other commentary. 
        """ 
    print(f"\n --- Generating info for:{product_query}--")
    try:
        response = await client.chat.completions.create(
            model = 'gpt-4.1-nano',
            messages=[
                {"role": "system", "content": system_prompt}, 
                {"role": "user", "content": f"Provide information for product: {product_query}"} 
            ],
            temperature=0.2,
            response_format={"type": "json_object"}
        )
        llm_output = response.choices[0].message.content
        print(f"Raw LLM Output: \n {llm_output}")

        # perform action
        if not is_json_and_conforms_to_schema(llm_output,PRODUCT_SCHEMA):
            print("Response is not valid JSON or does not match schema. Discarding")
            return None
        
        if contain_blacklisted_keywords(llm_output, CONTENT_BLACKLIST):
            print("Reponse contains blacklisted keywords. Discarding")
            return None
        
        print("Output Validate successfully!")
        return json.loads(llm_output)
    
    except Exception as e:
        print(f"An error Occurred: {e}")
        return None
    
async def self_critique_example(initial_response:str):
    print("\n --- Performing self critique ---")
    critique_prompt = f""" 
    You are a safety evaluation AI. Review the following text generated by another AI. 
    Is this text safe, polite, and does it avoid making harmful or biased statements? 
    If it is safe, respond with "SAFE". 
    If it is unsafe or problematic, respond with "UNSAFE" and briefly explain why. 
 
    Text to review: 
    --- 
    {initial_response} 
    --- 
    """
    try:
        critique_response = await client.chat.completions.create( 
                model="gpt-4o-mini",  
                messages=[ 
                    {"role": "system", "content": "You are an AI safety evaluator."}, 
                    {"role": "user", "content": critique_prompt} 
                ], 
                temperature=0.0, 
            )
        
        critique_result = critique_response.choices[0].message.content 
        print(f"Self-Critique Result: {critique_result}") 
        return critique_result.startswith("SAFE")
    except Exception as e:
        print(f"Critique error: {e}")
        return False
    
async def main():
    # # Valid product Output
    # valid_product_output = await generate_and_validate_product_info("SuperWidget Model X")
    # if valid_product_output:
    #     print(f" Validated Product Info: {valid_product_output}")

    # # bad non-JSON response
    # bad_llm_output_non_json = "Sorry, I cannot provide details for SuperGadget Z. It's a secret."
    # print(f"\n--- Validating simulated bad output (non-JSON): ---\n'{bad_llm_output_non_json}'")
    # if not is_json_and_conforms_to_schema(bad_llm_output_non_json, PRODUCT_SCHEMA):
    #     print(" Simulated bad output correctly identified as invalid.")

    # valid JSON but with blacklisted keywords
    # bad_llm_output_bad_keyword = '{"product_name": "TopSecretFile", "price": 100, "in_stock": true, "details": "Your new password is TopSecretFile"}'
    # print(f"\n--- Validating simulated bad output (bad keyword): ---\n'{bad_llm_output_bad_keyword}'")
    # if is_json_and_conforms_to_schema(bad_llm_output_bad_keyword, PRODUCT_SCHEMA):
    #     if contain_blacklisted_keywords(bad_llm_output_bad_keyword, CONTENT_BLACKLIST):
    #         print("Simulated bad output (bad keyword) correctly identified.")

    #Self-critique testing
    # potentially_problematic_response = "All politicians are the same, honestly."
    # is_safe = await self_critique_example(potentially_problematic_response)
    # print(f"\nIs '{potentially_problematic_response}' safe according to self-critique? {is_safe}")

    harmless_response = "The sky is blue."
    is_safe = await self_critique_example(harmless_response)
    print(f"Is '{harmless_response}' safe according to self-critique? {is_safe}")


if __name__ == "__main__":
    asyncio.run(main())

